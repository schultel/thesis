%==============================================================================
\chapter{Simulation}
\label{sec:sim}
%==============================================================================

As this thesis is about estimating the performance of the PINGU detector before
it is actually built, the estimate has to be based on simulations. In the
chapter on hand, the simulations used to generate the results reported in the
following chapter will be described in detail.

The chapter, as the simulation process, is divided into two sections.
Sec.~\ref{sec:sim_MCchain} will discuss the existing and well-established
IceCube Monte Carlo (MC) chain, which has been adopted for PINGU simulations.
Here, individual neutrino events are generated and their output of Cherenkov
photons is modelled. After propagating the photons through the ice, the
resulting hit pattern is processed through the standard reconstruction and event
selection specified in Secs.~\ref{sec:EvtReco} and \ref{sec:EvtSel},
respectively. The outcome of this event-by-event MC, \ie the effective
areas, reconstruction resolutions, and particle identification efficiencies for
all neutrino flavours, are then used as input for the second part of the
detector simulation.

The Parametric PINGU Analysis, \papa in short, was written specifically for the
rapid analysis of PINGU's neutrino mass hierarchy sensitivity including a
variety of systematic parameters. Since propagating these through the full
MC chain would be way too time-consuming, an effective detector
simulation was implemented that, instead of operating on individual events,
generates the expected event distributions directly, based on the detector
performance retrieved from the MC data. \papa will be described in
detail in Sec.~\ref{sec:papa}.

%==============================================================================
\section{The IceCube/PINGU Simulation Chain}
\label{sec:sim_MCchain}
%==============================================================================

\subsection{Event Generation}
\label{sec:MC_genie}

The first step in the MC chain is to model the interaction of an incoming
neutrino with a target nucleus and the resulting final state, the so-called
event generation. In the dedicated PINGU MC, this is carried out using the
GENIE (Generates Events for Neutrino Interaction Experiments) \cite{GENIE}
software package. This is already the first modification of the standard
IceCube MC chain, where NuGen \cite{NuGen}, an IceCube-specific neutrino
generator, is the default. Yet NuGen is laid out for high-energy neutrino
events where only deep inelastic scattering has to be considered as an
interaction process. In PINGU, however, the low GeV energy range is carrying the
interesting oscillation signal, and here the complex interplay between
quasi-elastic and deep-inelastic scattering as well as resonant processes have
to be taken into account (see Sec.~\ref{sec:XsecsGeV}). Since GENIE puts much
effort into modelling especially this energy range with great care and
validating it against experimental results, it is the natural choice for
generating PINGU events.

GENIE starts off with an isotropic flux of neutrinos of a given flavour
following a user-defined power-law distribution in energy (usually $\propto
E^{-1}$ or $E^{-2}$ for PINGU MC \cite{PINGU_MC}) on the surface of a
cylindrical generation volume well encompassing the full IceCube detector. Any
generated neutrino passing through the interaction volume, which is fully inside
the generation volume but still contains the detector as a whole, is forced to
interact inside this volume. The interaction type is chosen randomly from the
ones that are allowed and the event is assigned a weight $\mathcal{W}_i$
proportional to the particular interaction probability, taking into account the
generated energy spectrum. This weighting strategy makes it possible to
re-weight the generated events to any desired incoming flux $\Phi(E, \theta)$
later on. Then the actual weight is simply given by
\begin{equation}
 w_i = \frac{\Phi(E_i, \theta_i)\,\mathcal{W}_i}{N_\mathrm{evts}} \quad,
 \label{eqn:reweight}
\end{equation}
where $N_\mathrm{evts}$ is the total number of simulated events.

After the interaction mechanism has been determined, the interaction itself is
modelled in detail and all involved particles, from the initial neutrino and
nucleus over possible intermediate states to the final (meta-)stable particles
like pions or muons, are stored inside an \texttt{I3MCTree} object for further
processing. The reference to a tree comes from the fact that this object has
the structure of a multiply nested list, where every particle is the root of a
sub-tree (or branch) holding the particles created in its decay. The particles
are characterised by their identities, positions, four-momenta, and state (such
as 'initial', 'intermediate', or 'final'). 
Additional GENIE-specific information such as the number of generated events,
$N_\mathrm{evts}$, the size of the interaction volume, and others, are kept as
an \texttt{I3MCWeightDict} object.

\subsection{Particle Propagation}
\label{sec:MC_propagation}

The \texttt{I3MCTree} generated by GENIE is handed off to the mmc module
\cite{mmc}, which propagates the final state particles in the tree as well as
possible secondaries created in their decay further through the ice until they
have deposited all their energy. The particles involved here are mainly the
ones involved in the electromagnetic and hadronic showers discussed in
Sec.~\ref{sec:XsecsGeV}, \ie electrons, photons, and pions, respectively, as
well as muons and taus. The \texttt{I3MCTree} is extended with the outcome of
mmc, additional information being stored as \texttt{MMCTrackList} and passed on
to another module called clsim.

clsim \cite{clsim} is then used to generate the Cherenkov photons produced by
the particles propagating through the ice in the detector. Therefore, every
particle is converted into a series of steps of constant velocity $\beta =
v/c$, over which Cherenkov photons are emitted according to
(\ref{eqn:FrankTammWvl}). Usually this process of photon generation is handled
by the Geant4 package \cite{Geant4_1, Geant4_2}, but can also be done using an
effective parametrisation as well.

These photons are then propagated through the ice until they either get absorbed
or hit a DOM. Since photon propagation is a process that can well be run in 
parallel by multiple computation threads, clsim uses the publicly available
OpenCL library \cite{OpenCL} to outsource the calculations to GPUs, resulting
in a significant speedup compared to a simulation on CPUs. The photons that
have collided with a DOM are finally stored in a \texttt{I3MCHitSeriesMap},
containing their parent particle, wavelength, and position and angle of
incidence on the hit DOM. This information gets passed on to emulate the
response of the actual detector.

\subsection{Detector Response}
\label{sec:MC_detector}

In PINGU simulations, all DOMs are represented by an identical copy of the
standard DeepCore DOM, having a 35\,\% higher quantum efficiency than the
IceCube DOMs (see Sec.~\ref{sec:ICDOM}). This is only an approximation of the
actual PINGU DOMs, which currently only exist as prototypes, however it has
already become clear that especially the digitisation process will be
simplified. Yet as the PDOM design is not finalised, the DeepCore DOM is the
closest approximation at hand.

Before the response of the DOMs gets evaluated, noise hits from both thermal
electronic noise as well as radioactive decays inside the DOM and the
accompanying scintillation and fluorescence light are added to the
\texttt{HitSeries} with the vuvuzela module \cite{vuvuzela}. Then the
DOMLauncher module \cite{DOMLauncher} is called to generate the actual DOM
output.

\begin{figure}
\centering
  \subfloat[\label{fig:PMTjitter}]
    {\includegraphics[width=0.5\textwidth]{Jitter_Parameterization}}%\qquad
  \subfloat[\label{fig:SPEcharge}]
    {\includegraphics[width=0.5\textwidth]{TA0003}}
  \caption{Parametrisation of \protect\subref{fig:PMTjitter} the PMT transit
       time jitter distribution (in green) and \protect\subref{fig:SPEcharge}
       the single photo-electron charge distribution as used by the
       PMTResponseSimulator. Plots taken from \cite{PMTRes}}
\label{fig:PMTRes}
\end{figure}

The DOMLauncher first calls the PMTResponseSimulator submodule \cite{PMTRes} to
convert the single photo-electron produced at the PMT cathode by a DOM hit to a
charge pulse entering the DOM electronics. Here, the PMT transit time jitter is
applied, meaning that there is a spread in the times needed by the electron
avalanche developing on the PMT dynodes to pass through all amplification
stages. This distribution is shown in Fig.~\ref{fig:PMTjitter}. The
distribution of the amount of charge generated by a single photo-electron is
dominated by a Gaussian, per construction centred at the charge equivalent of
one photo-electron, but also contains a exponentially decreasing component of
small-amplitude pulses, as shown in Fig.~\ref{fig:SPEcharge}.

Once the main photon pulse has been processed, secondary effects like pre-,
late, and afterpulses are added. These result from photons hitting the first
dynode instead of the photocathode, scattered avalanche electrons hitting the
same dynode twice, and ionised residual gas atoms drifting onto the
photocathode, respectively, and are offset by a specific time window from the
main bunch of photo-electrons, but causally connected. Finally, saturation
effects are taken into account, which have to be considered for events of very
high energy or with a vertex in the close proximity of a single DOM.

The full PMT charge output, or waveform, is then passed to the main DOMLauncher
module, which propagates it through the DOM mainboard \cite{DOMLauncher}. First
a discriminator threshold and local coincidence logic are applied, deciding
whether a waveform gets digitised based on its strength and coincidence with a
hit on a neighbouring DOM. These steps will be removed in the actual PDOM since
advances in technology allow a continuous readout of the PMT waveform by a
single ADC instead of the multiple parallel ATWDs \cite{PDOM_Aachen}. Finally,
electronic noise in the digitisers and uncertainties in the time calibration are
added and a digitised representation of the waveform is created, which can then
be injected in the actual reconstruction chain described in
Secs.~\ref{sec:EvtReco} and \ref{sec:EvtSel}.

%==============================================================================
\section{The PaPA Code}
\label{sec:papa}
%==============================================================================

\subsection{Idea}
\label{sec:sim_idea}

As already discussed, the primary science goal of PINGU is the determination of 
the neutrino mass hierarchy, imprinted on the oscillation probability pattern 
of the atmospheric electron and muon neutrinos travelling through the Earth's 
matter potential (cf.~Sec.~\ref{sec:PINGUosc}). Observing this small effect 
requires a precise high-level analysis and a detailed knowledge of the detector 
performance. 

The observable in the mass hierarchy analysis is the distribution of arriving 
neutrinos in the ($E$, \coszen) plane. The shape of this distribution is 
determined not only by the mass hierarchy, but also by the true values of the 
other neutrino mixing parameters and the reconstruction efficiency and 
precision of the detector. All those---and especially their 
uncertainties---have to be taken into account when one wants to estimate the 
significance with which PINGU can determine the mass hierarchy.

The standard procedure to account for systematic uncertainties in IceCube is a 
brute force approach, where the parameters in question get increased and 
decreased by an amount corresponding to the estimated uncertainty. Then the 
simulation is re-run for each setting and the sensitivity re-evaluated. For the 
mass hierarchy measurement with PINGU, however, this strategy is not 
applicable. The reason for this is that in IceCube analyses usually only search 
for the existence of events passing a certain set of selection criteria---it is 
a detection experiment, where systematics in general have a much less severe 
impact compared to the search for a pattern in a large amount of data. Here, 
correlations between the systematics have to be taken into account as well, 
while in IceCube they can be considered independent. Additionally, the number 
of relevant systematic parameters is much higher for PINGU, since in IceCube 
oscillations usually do not have to be taken into account due to the much 
higher energy scale, and the detector itself is much better understood as it is 
already operating and hence could be studied and modelled in great detail.

Thus re-running the whole simulation chain for all of these parameters---we 
will go into detail about them in Sec.~\ref{sec:systematics}---is not feasible 
in a reasonable amount of time, especially since a comparatively large number 
of MC events---in the order of $10^6$---is needed to make a reliable estimate of
the sensitivity. In case of insufficient MC events, statistical fluctuations
will exceed the strength of the imprinted mass hierarchy pattern and bias the
calculated significance towards high values, as we will see in 
Sec.~\ref{sec:results_mcstats}.

The basic idea how to overcome this fundamental problem is to move away from an 
event-by-event Monte Carlo simulation and simulate the event histograms in 
($E,\,\coszen$) directly, based on parametrisations of the relevant detector 
quantities that are extracted from MC data. This is much faster than filling 
the histograms with individual events and has the benefit of quasi-infinite 
statistics since the parametrisations will in general be smooth functions, 
resulting in inherently smooth distributions. This strategy has been 
implemented in form of the \papa code, which will be described in detail in the 
following section.

A certain amount of MC events is needed still to produce the parametrisations 
that are the input to \papa, however it has been found that about 20000 events 
per neutrino flavour are sufficient to get stable fit results. This is at least
a factor of ten less than the number of events required to reach a stable 
result in Sec.~\ref{sec:results_mcstats}. Also, the MC does not need to be 
re-run for the different settings of the systematic parameters, as they are 
tuned within \papa.

\subsection{Implementation}
\label{sec:papa_code}

The \papa source code is available in the IceCube subversion repository 
\cite{papa_code}, with a detailed manual and version history maintained at the 
IceCube wiki pages \cite{papa_wiki}. It is written entirely in the python 
programming language \cite{python} and relies heavily on the \texttt{numpy} and 
\texttt{scipy} packages for numerical and scientific computing \cite{numpy, 
scipy}, with graphical output functionality based on the \texttt{matplotlib} 
library \cite{matplotlib}.

\begin{figure}
\centering
 \includegraphics[width=\textwidth]{PaPA_flowchart}
 \caption{Flow chart of the PaPA simulation chain}
\label{fig:papa_flowchart}
\end{figure}

\papa employs a staged processing procedure following the logical ordering and 
emulating the effects of the multiple physics processes that are involved in 
the actual measurement, as illustrated in Fig.~\ref{fig:papa_flowchart}. The 
code itself is split into two main parts that are run separately. The first 
part is the \texttt{PhysicsSimulation}, responsible only for the calculation of 
neutrino oscillation probabilities. This is by far the most time-consuming part 
as the Schr\"odinger equation for the oscillation in a varying matter 
potential (see Sec.~\ref{sec:matter_osc}) has to be solved numerically. But 
since the oscillations are independent from the specific settings of the PINGU 
detector, they can be calculated in advance and then be used multiple times 
when running the second part of \papa---the \texttt{DetectorSimulation}---for 
various detector settings.


\subsubsection{\texttt{PhysicsSimulation}}

In the physics simulation the neutrino oscillation probabilities in
the full three-flavour mode including matter effects are calculated, using
either NuCraft \cite{NuCraft} or AtmoWeights \cite{AtmoWeights} and assuming
the Preliminary Reference Earth Model (PREM) \cite{PREM} for the Earth's density
profile. The difference
between the two codes is that NuCraft is more versatile in the sense that it
can include the oscillation into sterile neutrino flavours, model the
generation heights of the neutrinos in the Earth's atmosphere in more detail,
and handle a varying electron density in the Earth while in AtmoWeights this
value is fixed to 0.5 electrons per nucleon. On the other hand, AtmoWeights is
much faster. After checking consistency of the two codes to the sub-per mill
level given the same input, AtmoWeights has been selected for the baseline
analysis due to its higher speed, since the additional options provided by
NuCraft are not essential for the study of the neutrino mass hierarchy.

The oscillation probabilities are calculated for $\nu_e$, $\bar\nu_e$, $\nu_\mu$
and $\bar\nu_\mu$ injection, such that twelve histograms of oscillation
probabilities are obtained:
\begin{itemize}
 \item $P(\nu_\alpha \to \nu_e/\bar\nu_e)$
 \item $P(\nu_\alpha \to \nu_\mu/\bar\nu_\mu)$
 \item $P(\nu_\alpha \to \nu_\tau/\bar\nu_\tau)$
\end{itemize}
with $\nu_\alpha \in \left[\nu_e,\bar\nu_e,\nu_\mu,\bar\nu_\mu\right]$.

% The relevant parameters for this step in the simulation chain are the physical
% ones, i.\,e.\ the mixing angles, mass splittings, hierarchy, and for
% implementation reasons also the energy scale (see below).

To properly sample the fast oscillations at low energies, \papa can be set up
to oversample by given factors in energy and zenith. This means that every
point at which the oscillation probabilities are calculated is replaced by a
number of evenly spaced points in the vicinity, and the resulting probabilities
are averaged over. If no oversampling is applied, the values at the bin
centres will be used.
% For our calculations, we used oversampling factors of eight in energy and two in
% zenith.

This part is the most time-consuming of the whole simulation as here all the
differential equations for neutrino propagation need to be solved numerically.
Using AtmoWeights, a full set of three oscillation parameters plus the energy
scale for four primaries at 15 test points each typically takes several hours
on a single CPU while with NuCraft this goes up to a few days. Fortunately, the
settings for the oscillations do not change when the detector changes, meaning
that the oscillation probabilities only have to be calculated once and then can
be re-used for all different detector settings.


\subsubsection{\texttt{DetectorSimulation}}

In the detector simulation, the pure oscillation probabilities calculated in the
physics simulation are converted into an actual detector response, taking into
account all detector related parameters. It consists of three major steps, as
depicted by the flow chart in Fig.~\ref{fig:papa_flowchart}. At each step, a set
of histograms in ($E$,\,\coszen) will be produced according to the
different signatures that have to be considered at the respective stage. Event
histograms at all simulation stages are shown in Fig.~\ref{fig:SimSteps}.


\paragraph{Truth Histograms}

The first step is to calculate the true rates of events triggering the
detector. Therefore, we generate spline fits to the azimuth-averaged
atmospheric neutrino flux tables by Honda et al.\ \cite{Honda,
HondaSP} and retrieve the fluxes for
$\nu_e/\bar\nu_e$ and $\nu_\mu/\bar\nu_\mu$. These fluxes are multiplied by the
oscillation probabilities $P(\nu_\alpha \to \nu_\beta)$ calculated in the
physics simulation to obtain the flux for each neutrino flavour at the
detector:
\begin{equation}
 \Phi_{\beta,\,\mathrm{det}} =
   \sum_\alpha \Phi_{\alpha,\,\mathrm{atm}}\ P(\nu_\alpha\to\nu_\beta)
\end{equation}

To obtain event rates, the neutrino fluxes are then multiplied by the effective
areas of PINGU for $\nue/\nuebar$, $\numu/\numubar$ and $\nutau/\nutaubar$ CC
interactions as well as $\nu_X/\bar\nu_X$ NC---the four different interaction
channels---after
background rejection cuts (cf.\ Sec.~\ref{sec:EvtSel}). The effective area, a
function of neutrino energy and zenith direction, is defined as the collection
area an ideal detector identifying every neutrino passing through it would need
to have in order to achieve the same event rate as PINGU. For a given
interaction channel, it can be calculated according to
\begin{equation}
 A_\mathrm{eff}(\mathrm{channel}) = \sum_\mathrm{target} N_\mathrm{target}\ 
   \sigma_{\mathrm{target},\,\mathrm{channel}}\ \epsilon_\mathrm{channel} \quad,
\end{equation}
summing over the possible targets, protons and neutrons. Here,
$N_\mathrm{target}$ stands for the total number of the respective targets
inside the detector volume, $\sigma_{\mathrm{target},\,\mathrm{channel}}$ is
the interaction cross section in the given channel, and
$\epsilon_\mathrm{channel}$ is the efficiency with which interactions occurring
inside the detector actually pass the triggering, reconstruction, and event
selection process. Although the selection efficiency is usually on the order of
90\,\% \cite{cutsV5}, the effective area is much smaller than the geometrical
size of the detector as the interaction cross sections for neutrinos are
extremely small (see Sec.~\ref{sec:Xsecs}) and hence most neutrinos pass the
detector without any interaction.

The number of expected events per lifetime $t$ in a given channel can thus be
calculated by
\begin{eqnarray}
 N_\mathrm{\nu_\alpha\ \mathrm{CC}} &=&
   A_\mathrm{eff}(\nu_\alpha\ \mathrm{CC})\ \Phi_{\alpha,\,\mathrm{det}}\ t \\
 N_\mathrm{\nu_X\ \mathrm{NC}} &=&
   A_\mathrm{eff}(\nu_X\ \mathrm{NC}) \sum_\alpha \Phi_{\alpha,\,\mathrm{det}}\
t
\end{eqnarray}
for the three charged and one neutral current channels, respectively.
Finally, the histograms for neutrinos and anti-neutrinos in the same
interaction channels are added since PINGU will not be able to distinguish
between them. Also in terms of reconstruction quality and flavour
identification (see below) there is no difference between neutrinos and
anti-neutrinos. 

Since both the fluxes and the effective areas are functions of $E$ and \coszen,
the result of this simulation step are four 2D histograms, one each for the
four channels $\nue+\nuebar$ CC, $\numu+\numubar$ CC, $\nutau+\nutaubar$ CC, and
$\nu_X+\bar\nu_X$ NC, examples of which are shown in the second row of
Fig.~\ref{fig:SimSteps}.

\paragraph{Reconstructed Histograms}

In the next step, event reconstruction is inflicted on the histograms that up
to now contain true neutrino energies and directions. This is done by smearing
the true histograms with a kernel\footnote{A \emph{smearing kernel} can be
interpreted as a binned point spread function.} whose shape depends on the true
energy and zenith angle of the event, i.\,e.\ the position in the histogram.
There are two different ways how these kernels can be handed over to \papa:

\begin{enumerate}[(a)]
 \item Parametrised point spread functions.\\ These have to be fitted from MC
  data and generally take the form of a double Gaussian for both energy and
  $\cos\vartheta$ to properly account for the tails:
  \begin{equation}
   \mathrm{PSF} = f\cdot \exp\left(-\frac{(x-\mu_1)^2}{\sigma_1^2}\right)
                  + (1-f)\cdot \exp\left(-\frac{(x-\mu_2)^2}{\sigma_2^2}\right)
   \label{eqn:reco_param}
  \end{equation}
  The five parameters (two mean values $\mu$, two widths $\sigma$, and the
  relative normalisation $f$) are supplied to the code as a function of true
  neutrino energy.\\
  Here it can happen that events are ``leaking'' out of the histograms. At
  the edges of the energy range this effect can be neglected since the event
  rates are small anyways (due to low flux/small effective area). In the
  directional dimension, losing events at the horizon is accepted since
  mis-reconstructed events will be also be lost in the real experiment due to
  cutting the analysis at the horizon. Events that would be lost at the zenith
  are reflected back into the histogram, as events migrating ``over the
  zenith'' will flip the sign of their azimuth angle (which is irrelevant) and
  move back to larger values of the zenith angle.
 \item Tabulated point spread functions.\\ If the MC statistics is sufficient,
  i.\,e.\ in the order of $10^6$ events per flavour, the resolutions can be
  retrieved directly from the MC data. This means creating a 4D histogram of
  true vs.\ reconstructed ($E$, \coszen), which is nothing else than
  the individual smearing kernel for each bin in true ($E$, \coszen).
  \end{enumerate}
Option (a) will be used primarily in this study, since for the PINGU geometry
V36 that will be the baseline for all analyses, the amount of MC events is
insufficient for option (b). Also, the parametrised reconstruction resolutions
make it possible to investigate the effects of an improved resolution within a
well-defined metric in Sec.~\ref{sec:wom_effect}.
For the earlier PINGU geometry V15, however, enough MC statistics exists to
follow option (b) for the event reconstruction stage.
Sec.~\ref{sec:results_mcstats} will report on this and also demonstrate the
effect that an insufficient MC sample has on the calculated NMH sensitivity.

Example histograms after the reconstruction stage---still in the four
interaction channels already present after the previous stage---are shown in
the third row of Fig.~\ref{fig:SimSteps}.

\paragraph{Analysis Histograms}

% \begin{figure}
%  \centering
%  $\begin{array}{cc}
%    \mathrm{Tracks} &
%    \mathrm{Cascades}\\
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/1_un_oscillated_trck.pdf} &
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/1_un_oscillated_cscd.pdf}\\
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/2_oscillated_trck.pdf} &
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/2_oscillated_cscd.pdf}\\
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/3_reconstructed_trck.pdf} &
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/3_reconstructed_cscd.pdf}\\
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/4_classified_trck.pdf} &
%    \includegraphics[width=0.5\columnwidth]
%      {figures/simulation_steps/4_classified_cscd.pdf}
%   \end{array}$
%  \caption{Event counts in one year of PINGU livetime at the different
%   simulation stages, assuming normal mass hierarchy.}
%  \label{fig:SimSteps}
% \end{figure}

Up to now, the histograms are still divided into the four ``effective flavour''
channels, $\nu_e$, $\nu_\mu$, $\nu_\tau$ CC and $\nu_X$ NC, each including
anti-neutrinos. The final step in the detector simulation is to apply the
particle flavour identification (PID). However, current tools for particle ID
can only distinguish events with an outgoing muon signature (only present in
$\nu_\mu$ CC interactions), which are classified as tracks, from other events
(cascades), as already detailed above (cf.\ Sec.~\ref{sec:cuts_PID}).

So for every interaction channel, two analytic functions of the neutrino
energy have to be supplied to \papa, giving the probabilities of identifying a
neutrino of that channel and energy as either track or cascade. If those two
probabilities add up to less than one, the remaining events are assigned to an
``unidentified'' channel that can be used for the analysis as well.

For the baseline settings (Sec.~\ref{sim_input}), the PID is a binary decision,
meaning that there are no unidentified events. In
Sec.~\ref{sec:results_includeunkn}, the effects of an event selection producing
cascade and track samples of higher purity plus an unidentified set of events
will be discussed.

After this final stage, all key features of a real experiment (triggering,
event selection, reconstruction, and flavour identification) have been taken
into account, and the analysis histograms are our best estimate of the outcome
of an actual PINGU physics run. Example histograms after this final step, also
referred to as analysis histograms, are shown in the bottom row of
Fig.~\ref{fig:SimSteps} and will also be used throughout Chapter \ref{sec:ana}
to illustrate the various effects studied.


\subsection{Systematic Parameters}
\label{sec:systematics}

The main benefit of the fast simulation is that it makes the inclusion of
the many systematic parameters feasible, which are needed to correctly evaluate
the mass hierarchy significance. The parameters that were considered in this
thesis can be divided into two groups. First the \emph{physics} parameters,
\ie the neutrino oscillation parameters: three mixing angles, two mass
splittings, the CP violating phase and of course the sign of the NMH. The best
fit values and priors for the mixing angles and mass splittings as listed in
Tab.~\ref{tab:fiducial_osc} are taken from \cite{Fogli}, where we have assumed
inverted NMH as fiducial value. The CP violating phase $\delta_\mathrm{CP}$ is
set to a fiducial value of zero.

After having checked that their impact is negligible, the so-called solar mixing
parameters \dm{21} and \thet{12} as well as the CP phase will not be left free
to vary any more in order to save computation time.

The mass hierarchy needs special treatment since it is a binary quantity,
however the Fisher matrix formalism (see Sec.~\ref{sec:fisher}), which will be
used to quickly evaluate the simulation outcome, can only treat continuous
parameters. Thus the NMH
needs to be converted into a continuous variable. This is achieved by
calculating the oscillation probabilities assuming normal and inverted NMH. We
then introduce a hierarchy parameter $0\leq h\leq1$ that allows for a continuous
transition between normal (NH) and inverted (IH) hierarchy:
\begin{equation}
 P(h) = h P_\mathrm{NH} + (1-h) P_\mathrm{IH}\quad,
 \label{eqn:hierarchy_parameter}
\end{equation}
where $P_\mathrm{NH}$ and $P_\mathrm{IH}$ are the oscillation probabilities for
a given value of ($E,\,\coszen$) for normal and inverted mass hierarchy,
respectively, while all other oscillation parameters are kept at there fiducial
values.
The rationale is that while in theory any value of $h$ other than $0$ or $1$ is
unphysical, in practice this definition allows to assess the deviation from the
physical points, as long as these are close to the fiducial values ($1$ in our
case). It is such a measure of the hierarchy \textit{likeness}, similar to a
likelihood\footnote{It can also be interpreted as a measure of the orthogonal
distance of the hyperplanes in parameter spaces defined by normal and inverted
hierarchy}.

After propagating the oscillation probabilities through the detector simulation

If no other parameters are present, this description is,, mathematically
equivalent to a bin-wise $\Delta\chi^2$ sum as defined by Akhmedov et al.\
\cite{Akhmedov}:
\begin{equation}
 \Delta\chi^2 = \frac{N_\mathrm{NH}-N_\mathrm{IH}}{\sqrt{N_\mathrm{NH}}}
\end{equation}
with the significance
\begin{equation}
 \mathcal{S}_{\chi^2} = \sqrt{\sum_\mathrm{bins} \left(\Delta\chi^2\right)^2}
 = \sqrt{\sum_\mathrm{bins}
     \frac{(N_\mathrm{NH}-N_\mathrm{IH})^2}{N_\mathrm{NH}} }\quad.
\end{equation}
In this case, rom the Fisher Matrix we get the identical significance of
\begin{equation}
 \mathcal{S}_\mathrm{Fisher} = \frac{1}{\sigma_h} = \frac{1}{\sqrt{\sigma_h^2}}
 = \frac{1}{\sqrt{(\mathcal{F})^{-1}_{hh}}}
 = \sqrt{\mathcal{F}_{hh}}
\end{equation}
with
\begin{equation}
 \mathcal{F}_{hh} = \sum_\mathrm{bins} \frac{1}{\sigma_b^2}
   \frac{\partial N}{\partial h} \frac{\partial N}{\partial h}
  = \sum_\mathrm{bins} \frac{1}{N_\mathrm{NH}}(N_\mathrm{NH}-N_\mathrm{IH})^2
  \quad.
\end{equation}



The second set of parameters are the ones related to the detector, i.\,e.\ the
nuisance parameters. They include parametrisations of reconstruction errors,
uncertainty on the energy scale or effective detector mass, etc.  A list of all
oscillation parameters with their fiducial values and possible priors is shown
in Tab.~\ref{tab:params}. A detailed description of the detector parameters
will follow in Sec.\ref{sec:detector}.



The systematic uncertainties on the detector response enter at this stage:
\begin{description}
 \item[The energy scale $\mathbf{s_E}$] describes a potential scaling error
  between true and reconstructed neutrino energy. Such a mis-scaling, i.\,e.\
  an under- or overestimation of the neutrino energy can be the result of a
  wrong value for e.\,g.\ PMT efficiency, absorption in bulk or hole ice etc.
  We implement it in a way that the true values of the energy bins that are fed
  to NuCraft are scaled w.\,r.\,t.\ the energy bins in the analysis according to
  \begin{equation}
   E_\mathrm{true} = s_E\cdot E_\mathrm{ana}\quad.
  \end{equation}

% Diese Scale müssten wir eigentlich auch in den nächsten Schritten
% berücksichtigen, oder? Machen wir das?
%
% -> NEIN
% Aber müssten wir das wirklich? Wenn wir die ganzen Auflösungen, PID etc
% mitschieben, sind wir doch am Ende wieder da, wo wir angefangen haben, oder?

 \item[The effective area scale $\mathbf{s_{A_\mathrm{eff}}}$] opens the
  possibility that the effective area scales differently with energy than
  assumed in Fig.~\ref{fig:aeffs}. It will also partially compensate for a
  potential error in the spectral index of the atmospheric flux, since it
  effectively alters the total number of events per energy bin as a function of
  energy. A constant scaling of the effective area (or the flux normalisation)
  is not taken into account, since it would only change the total number of
  events and therefore the livetime needed to reach a given significance, but is
  not correlated with any other parameters.

 \item[The relative effective area normalisations] for neutrinos
  and anti-neutrinos, $n_{A_\mathrm{eff},\nu}$ and $n_{A_\mathrm{eff},\bar\nu}$,
  account for a possible error in the (anti-)neutrino-nucleon cross-sections,
  which effectively alters the effective areas\footnote{Inflicting this
  re-normalisation on the atmospheric fluxes instead of the effective areas
  would have the same effect. Since the effects are completely degenerate, we do
  not treat the flux normalisation as a parameter.}. In total, the effective
  areas are then given by
  \begin{equation}
   A_\mathrm{eff,\,true} = A_\mathrm{eff}
                           \cdot\left(1+s_{A_\mathrm{eff}}\cdot E\right)
                           \cdot n_{A_\mathrm{eff},\nu/\bar\nu}\quad.
  \end{equation}
\end{description}


%TODO: still ignore those parameters? We can do that if we use the reco tables
% Two more systematic parameters can enter at this stage, if parametrised point
% spread functions are used for event reconstruction:
% \begin{description}
%  \item[The energy reconstruction scaling $\mathbf{s_{\Delta E}}$] reflects the
%   fact that the parametrisations of the energy reconstruction might under- or
%   overestimate the actual resolution. This is implemented by simply scaling
% the
%   widths in (\ref{eqn:reco_param}) by this factor:
%   \begin{equation}
%    \sigma_i' = s_{\Delta E}\cdot\sigma_i
%   \end{equation}
%   Note that for simplicity the scaling is the same for both of the Gaussians.
%  \item[The zenith reconstruction scaling $\mathbf{s_{\Delta\cos\vartheta}}$]
%   is modelled in the same way as the energy reconstruction scaling:
%   \begin{equation}
%    \sigma_i' = s_{\Delta\cos\vartheta}\cdot\sigma_i
%   \end{equation}
% \end{description}

Two more systematic parameters enter at the flavour identification stage:
\begin{description}
 \item[The FID scaling $\mathbf{s_\mathrm{FID}}$] simply scales the probability
  to detect a track signature up or down, accounting for the possibility to
  mis-estimate the overall effectivity of the flavour ID algorithm.
 \item[The FID offset $\mathbf{\Delta_\mathrm{FID}}$] shifts the whole flavour
  ID function in energy to open the possibility that FID might become effective
  at higher or lower energies than fitted from MonteCarlo data.
\end{description}
The actual FID function will hence be
\begin{equation}
 P_\mathrm{ID}'(E) = s_\mathrm{FID}\cdot P_\mathrm{ID}(E-\Delta_\mathrm{FID})
   \quad.
\end{equation}

