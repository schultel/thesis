%==============================================================================
\chapter{Simulation}
\label{sec:sim}
%==============================================================================

As this thesis is about estimating the performance of the PINGU detector before
it is actually built, the estimate has to be based on simulations. In the
chapter on hand, the simulations used to generate the results reported in the
following chapter will be described in detail.

The chapter, as the simulation process, is divided into two sections.
Sec.~\ref{sec:sim_MCchain} will discuss the existing and well-established
IceCube Monte Carlo (MC) chain, which has been adopted for PINGU simulations.
Here, individual neutrino events are generated and their output of Cherenkov
photons is modelled. After propagating the photons through the ice, the
resulting hit pattern is processed through the standard reconstruction and event
selection specified in Secs.~\ref{sec:EvtReco} and \ref{sec:EvtSel},
respectively. The outcome of this event-by-event MC, \ie the effective
areas, reconstruction resolutions, and particle identification efficiencies for
all neutrino flavours, are then used as input for the second part of the
detector simulation.

The Parametric PINGU Analysis, \papa in short, was written specifically for the
rapid analysis of PINGU's neutrino mass hierarchy sensitivity including a
variety of systematic parameters. Since propagating these through the full
MC chain would be way too time-consuming, an effective detector
simulation was implemented that, instead of operating on individual events,
generates the expected event distributions directly, based on the detector
performance retrieved from the MC data. \papa will be described in
detail in Sec.~\ref{sec:papa}.

%==============================================================================
\section{The IceCube/PINGU Simulation Chain}
\label{sec:sim_MCchain}
%==============================================================================

\subsection{Event Generation}
\label{sec:MC_genie}

The first step in the MC chain is to model the interaction of an incoming
neutrino with a target nucleus and the resulting final state, the so-called
event generation. In the dedicated PINGU MC, this is carried out using the
GENIE (Generates Events for Neutrino Interaction Experiments) \cite{GENIE}
software package. This is already the first modification of the standard
IceCube MC chain, where NuGen \cite{NuGen}, an IceCube-specific neutrino
generator, is the default. Yet NuGen is laid out for high-energy neutrino
events where only deep inelastic scattering has to be considered as an
interaction process. In PINGU, however, the low GeV energy range is carrying the
interesting oscillation signal, and here the complex interplay between
quasi-elastic and deep-inelastic scattering as well as resonant processes have
to be taken into account (see Sec.~\ref{sec:XsecsGeV}). Since GENIE puts much
effort into modelling especially this energy range with great care and
validating it against experimental results, it is the natural choice for
generating PINGU events.

GENIE starts off with an isotropic flux of neutrinos of a given flavour
following a user-defined power-law distribution in energy (usually $\propto
E^{-1}$ or $E^{-2}$ for PINGU MC \cite{PINGU_MC}) on the surface of a
cylindrical generation volume well encompassing the full IceCube detector. Any
generated neutrino passing through the interaction volume, which is fully inside
the generation volume but still contains the detector as a whole, is forced to
interact inside this volume. The interaction type is chosen randomly from the
ones that are allowed and the event is assigned a weight $\mathcal{W}_i$
proportional to the particular interaction probability, taking into account the
generated energy spectrum. This weighting strategy makes it possible to
re-weight the generated events to any desired incoming flux $\Phi(E, \theta)$
later on. Then the actual weight is simply given by
\begin{equation}
 w_i = \frac{\Phi(E_i, \theta_i)\,\mathcal{W}_i}{N_\mathrm{evts}} \quad,
 \label{eqn:reweight}
\end{equation}
where $N_\mathrm{evts}$ is the total number of simulated events.

After the interaction mechanism has been determined, the interaction itself is
modelled in detail and all involved particles, from the initial neutrino and
nucleus over possible intermediate states to the final (meta-)stable particles
like pions or muons, are stored inside an \texttt{I3MCTree} object for further
processing. The reference to a tree comes from the fact that this object has
the structure of a multiply nested list, where every particle is the root of a
sub-tree (or branch) holding the particles created in its decay. The particles
are characterised by their identities, positions, four-momenta, and state (such
as 'initial', 'intermediate', or 'final'). 
Additional GENIE-specific information such as the number of generated events,
$N_\mathrm{evts}$, the size of the interaction volume, and others, are kept as
an \texttt{I3MCWeightDict} object.

\subsection{Particle Propagation}
\label{sec:MC_propagation}

The \texttt{I3MCTree} generated by GENIE is handed off to the mmc module
\cite{mmc}, which propagates the final state particles in the tree as well as
possible secondaries created in their decay further through the ice until they
have deposited all their energy. The particles involved here are mainly the
ones involved in the electromagnetic and hadronic showers discussed in
Sec.~\ref{sec:XsecsGeV}, \ie electrons, photons, and pions, respectively, as
well as muons and taus. The \texttt{I3MCTree} is extended with the outcome of
mmc, additional information being stored as \texttt{MMCTrackList} and passed on
to another module called clsim.

clsim \cite{clsim} is then used to generate the Cherenkov photons produced by
the particles propagating through the ice in the detector. Therefore, every
particle is converted into a series of steps of constant velocity $\beta =
v/c$, over which Cherenkov photons are emitted according to
(\ref{eqn:FrankTammWvl}). Usually this process of photon generation is handled
by the Geant4 package \cite{Geant4_1, Geant4_2}, but can also be done using an
effective parametrisation as well.

These photons are then propagated through the ice until they either get absorbed
or hit a DOM. Since photon propagation is a process that can well be run in 
parallel by multiple computation threads, clsim uses the publicly available
OpenCL library \cite{OpenCL} to outsource the calculations to GPUs, resulting
in a significant speedup compared to a simulation on CPUs. The photons that
have collided with a DOM are finally stored in a \texttt{I3MCHitSeriesMap},
containing their parent particle, wavelength, and position and angle of
incidence on the hit DOM. This information gets passed on to emulate the
response of the actual detector.

\subsection{Detector Response}
\label{sec:MC_detector}

In PINGU simulations, all DOMs are represented by an identical copy of the
standard DeepCore DOM, having a 35\,\% higher quantum efficiency than the
IceCube DOMs (see Sec.~\ref{sec:ICDOM}). This is only an approximation of the
actual PINGU DOMs, which currently only exist as prototypes, however it has
already become clear that especially the digitisation process will be
simplified. Yet as the PDOM design is not finalised, the DeepCore DOM is the
closest approximation at hand.

Before the response of the DOMs gets evaluated, noise hits from both thermal
electronic noise as well as radioactive decays inside the DOM and the
accompanying scintillation and fluorescence light are added to the
\texttt{HitSeries} with the vuvuzela module \cite{vuvuzela}. Then the
DOMLauncher module \cite{DOMLauncher} is called to generate the actual DOM
output.

\begin{figure}
\centering
  \subfloat[\label{fig:PMTjitter}]
    {\includegraphics[width=0.5\textwidth]{Jitter_Parameterization}}%\qquad
  \subfloat[\label{fig:SPEcharge}]
    {\includegraphics[width=0.5\textwidth]{TA0003}}
  \caption{Parametrisation of \protect\subref{fig:PMTjitter} the PMT transit
       time jitter distribution (in green) and \protect\subref{fig:SPEcharge}
       the single photo-electron charge distribution as used by the
       PMTResponseSimulator. Plots taken from \cite{PMTRes}}
\label{fig:PMTRes}
\end{figure}

The DOMLauncher first calls the PMTResponseSimulator submodule \cite{PMTRes} to
convert the single photo-electron produced at the PMT cathode by a DOM hit to a
charge pulse entering the DOM electronics. Here, the PMT transit time jitter is
applied, meaning that there is a spread in the times needed by the electron
avalanche developing on the PMT dynodes to pass through all amplification
stages. This distribution is shown in Fig.~\ref{fig:PMTjitter}. The
distribution of the amount of charge generated by a single photo-electron is
dominated by a Gaussian, per construction centred at the charge equivalent of
one photo-electron, but also contains a exponentially decreasing component of
small-amplitude pulses, as shown in Fig.~\ref{fig:SPEcharge}.

Once the main photon pulse has been processed, secondary effects like pre-,
late, and afterpulses are added. These result from photons hitting the first
dynode instead of the photocathode, scattered avalanche electrons hitting the
same dynode twice, and ionised residual gas atoms drifting onto the
photocathode, respectively, and are offset by a specific time window from the
main bunch of photo-electrons, but causally connected. Finally, saturation
effects are taken into account, which have to be considered for events of very
high energy or with a vertex in the close proximity of a single DOM.

The full PMT charge output, or waveform, is then passed to the main DOMLauncher
module, which propagates it through the DOM mainboard \cite{DOMLauncher}. First
a discriminator threshold and local coincidence logic are applied, deciding
whether a waveform gets digitised based on its strength and coincidence with a
hit on a neighbouring DOM. These steps will be removed in the actual PDOM since
advances in technology allow a continuous readout of the PMT waveform by a
single ADC instead of the multiple parallel ATWDs \cite{PDOM_Aachen}. Finally,
electronic noise in the digitisers and uncertainties in the time calibration are
added and a digitised representation of the waveform is created, which can then
be injected in the actual reconstruction chain described in
Secs.~\ref{sec:EvtReco} and \ref{sec:EvtSel}.

%==============================================================================
\section{The PaPA Code}
\label{sec:papa}
%==============================================================================

\subsection{Idea}
\label{sec:sim_idea}

As already discussed, the primary science goal of PINGU is the determination of 
the neutrino mass hierarchy, imprinted on the oscillation probability pattern 
of the atmospheric electron and muon neutrinos travelling through the Earth's 
matter potential (cf.~Sec.~\ref{sec:PINGUosc}). Observing this small effect 
requires a precise high-level analysis and a detailed knowledge of the detector 
performance. 

The observable in the mass hierarchy analysis is the distribution of arriving 
neutrinos in the ($E$, \coszen) plane. The shape of this distribution is 
determined not only by the mass hierarchy, but also by the true values of the 
other neutrino mixing parameters and the reconstruction efficiency and 
precision of the detector. All those---and especially their 
uncertainties---have to be taken into account when one wants to estimate the 
significance with which PINGU can determine the mass hierarchy.

The standard procedure to account for systematic uncertainties in IceCube is a 
brute force approach, where the parameters in question get increased and 
decreased by an amount corresponding to the estimated uncertainty. Then the 
simulation is re-run for each setting and the sensitivity re-evaluated. For the 
mass hierarchy measurement with PINGU, however, this strategy is not 
applicable. The reason for this is that in IceCube analyses usually only search 
for the existence of events passing a certain set of selection criteria---it is 
a detection experiment, where systematics in general have a much less severe 
impact compared to the search for a pattern in a large amount of data. Here, 
correlations between the systematics have to be taken into account as well, 
while in IceCube they can be considered independent. Additionally, the number 
of relevant systematic parameters is much higher for PINGU, since in IceCube 
oscillations usually do not have to be taken into account due to the much 
higher energy scale, and the detector itself is much better understood as it is 
already operating and hence could be studied and modelled in great detail.

Thus re-running the whole simulation chain for all of these parameters---we 
will go into detail about them in Sec.~\ref{sec:systematics}---is not feasible 
in a reasonable amount of time, especially since a comparatively large number 
of MC events is needed to make a reliable estimate of the sensitivity. The MC 
sample has to exceed the expected number of recorded by roughly a factor of 
ten. In case of insufficient MC events, statistical fluctuations will exceed 
the strength of the imprinted mass hierarchy pattern and bias the calculated 
significance towards high values, as we will see in 
Sec.~\ref{sec:results_mcstats}.

The basic idea how to overcome this fundamental problem is to move away from an 
event-by-event Monte Carlo simulation and simulate the event histograms in 
($E,\,\coszen$) directly, based on parametrisations of the relevant detector 
quantities that are extracted from MC data. This is much faster than filling 
the histograms with individual events and has the benefit of quasi-infinite 
statistics since the parametrisations will in general be smooth functions, 
resulting in inherently smooth distributions. This strategy has been 
implemented in form of the \papa code, which will be described in detail in the 
following section.

A certain amount of MC events is needed still to produce the parametrisations 
that are the input to \papa, however it has been found that about 20000 events 
per neutrino flavour are sufficient to get stable fit results. This is at least
a factor of ten less than the number of events required to reach a stable 
result in Sec.~\ref{sec:results_mcstats}. Also, the MC does not need to be 
re-run for the different settings of the systematic parameters, as they are 
tuned within \papa.

\subsection{Implementation}
\label{sec:papa_code}

The \papa source code is available in the IceCube subversion repository 
\cite{papa_code}, with a detailed manual and version history maintained at the 
IceCube wiki pages \cite{papa_wiki}. It is written entirely in the python 
programming language \cite{python} and relies heavily on the \texttt{numpy} and 
\texttt{scipy} packages for numerical and scientific computing \cite{numpy, 
scipy}, with graphical output functionality based on the \texttt{matplotlib} 
library \cite{matplotlib}.

\begin{figure}
\centering
 \includegraphics[width=\textwidth]{PaPA_flowchart}
 \caption{Flow chart of the PaPA simulation chain}
\label{fig:papa_flowchart}
\end{figure}

\papa employs a staged processing procedure following the logical ordering and 
emulating the effects of the multiple physics processes that are involved in 
the actual measurement, as illustrated in Fig.~\ref{fig:papa_flowchart}. The 
code itself is split into two main parts that are run separately. The first 
part is the \texttt{PhysicsSimulation}, responsible only for the calculation of 
neutrino oscillation probabilities. This is by far the most time-consuming part 
as the Schr\"odinger equation for the oscillation in a varying matter 
potential (see Sec.~\ref{sec:matter_osc}) has to be solved numerically. But 
since the oscillations are independent from the specific settings of the PINGU 
detector, they can be calculated in advance and then be used multiple times 
when running the second part of \papa---the \texttt{DetectorSimulation}---for 
various detector settings.

\subsection{Systematic Parameters}
\label{sec:systematics}
