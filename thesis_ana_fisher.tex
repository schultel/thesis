%==============================================================================
\chapter{Analysis}
\label{sec:ana}
%==============================================================================

Now that the tools and theoretical foundations needed for the simulation of an
PINGU physics run have been presented and described, the next step is the
actual calculation of PINGU's physics capabilities, in particular its
sensitivity for the neutrino mass hierarchy. This will be done using the Fisher
information matrix, a tool to quickly evaluate the covariance matrix of an
experiment where a large number of systematic uncertainties has to be taken
into account, which will be introduced in Sec.~\ref{sec:fisher}.

Thereafter, the simulation input that was used for the generation of the
results is documented (Sec.~\ref{sec:sim_input}) before finally the results
themselves can be presented and discussed in Sec.~\ref{sec:results_baseline}.
Looking even more into the future, in Sec.~\ref{sec:om_effects} 
the possible benefits of instrumenting PINGU with the next generation optical
modules described in Sec.~\ref{sec:Gen2DOM} will be estimated. Finally, one can
make use of the
Fisher matrix's ability to easily combine the results of different experiments
measuring the same physical effect to evaluate the benefits gained from the
joint analysis of PINGU and other neutrino oscillation experiments, in
particular JUNO \cite{JUNO} (Sec.~\ref{sec:JUNO}).


%==============================================================================
\section{Fisher Information Matrix}
\label{sec:fisher}
%==============================================================================

The Fisher information matrix, or just Fisher matrix in the following, provides
a way to estimate the full covariance matrix of an experiment and therefore the
accuracy of its intended measurement in advance. It has been widely used,
especially in cosmology \cite{Fisher_first}, but also in neutrino
astrophysics \cite{MarekDiffuse}. For a detailed discussion see \eg
\cite{Fisher_first, DETF, DETF2}.

The experiment that is to be modelled is characterised by two sets of variables:
\begin{description}
 \item[Observables] are the variables $f_n$ that will actually be measured by
  the experiment. In the case of PINGU, these are the expected event counts 
  binned in energy and zenith angle. Since the partial derivatives of the
  observables \wrt the parameters enter the calculation of the Fisher matrix,
  their dependence on the parameters has to be known either analytically or (as
  in this analysis) from simulations, such that these derivatives can be
  determined in an analytical or numerical way.
 \item[Parameters] are the variables $p_i$ that will be extracted from the
  measurement (\ie from the observables). These are the physical parameters
  which are the actual target of the experiment as well as nuisance parameters
  required to account for systematic uncertainties.
\end{description}
Then the Fisher matrix is defined as:
\begin{equation}
 \mathcal{F}_{ij} = \sum_n \frac{1}{\sigma_n^2} \frac{\partial f_n}{\partial
p_i} \frac{\partial f_n}
 {\partial p_j}\bigg|_\mathrm{fid.\,model}
 \label{eqn:fisher_def}
\end{equation}
Here the $\sigma_n$ denote the errors on the measurement of the observables.
Since in this analysis, these are the expected numbers of events in the bins $n$
of the analysis histograms (cf.\ Sec.~\ref{sec:papa_code}), $f_n = N_n$, one
can apply Poissonian statistics where the errors are simply given by the square
root of the number of events:
\begin{equation}
 \sigma_n = \sqrt{f_n} = \sqrt{N_n}
\end{equation}
The derivatives in (\ref{eqn:fisher_def}) are evaluated at the set of ''true''
values of $p_i$ that are used as an input for the simulation. This set of
parameters $p_i$ is called the \emph{fiducial model} and is chosen with the best
existing knowledge.

\subsection{Properties}

Once the Fisher matrix has been constructed, the covariance
matrix of the experiment is obtained by inverting the Fisher matrix. Then the
full errors of the parameters $\sigma_i^{\rm full}$ and their correlations can
be read from the covariance matrix $\mathcal{S}$:
\begin{equation}
 \sigma_i = \sigma_i^\mathrm{full} = \sqrt{\mathcal{S}_{ii}}
                        = \sqrt{\left(\mathcal{F}^{-1}\right)_{ii}}
 \label{eqn:sigma_full}
\end{equation}
Also the purely statistical error of a given parameter $\sigma_i^\mathrm{stat}$
(\ie the error that is obtained assuming all other parameters are fixed) can be
calculated from the corresponding diagonal element of the Fisher matrix:
\begin{equation}
 \sigma_i^\mathrm{stat} = \sqrt{\left(\mathcal{F}_{ii}\right)^{-1}}
\label{eqn:sigma_stat}
\end{equation}

External constraints on parameters, so-called \emph{priors}, can easily be
incorporated as well. If parameter $i$ has an prior $\sigma_i^\mathrm{ext}$, it
can simply be added to the corresponding diagonal element of the Fisher matrix:
\begin{equation}
 \mathcal{F}_{ii}^\mathrm{prior} = \mathcal{F}_{ii} +
     \left(\frac{1}{\sigma_i^\mathrm{ext}}\right)^2
\end{equation}
To fix one parameter completely, the corresponding row and column are removed
from the Fisher matrix before inversion.

The Fisher matrix allows not only to read off the full uncertainty of any
parameter and all its correlations with other parameters, but also decompose the
full error into the purely statistical part (\ref{eqn:sigma_stat}) and the
contribution arising from the uncertainties of the other parameters, denoted as
\emph{systematic} error $\sigma_i^\mathrm{syst}$ below:
\begin{equation}
 \sigma^\mathrm{syst}_i =
         \sqrt{\sigma_i^2 - \left(\sigma^\mathrm{stat}_i\right)^2}\quad,
 \label{eqn:syst_error}
\end{equation}
where $\sigma_i$ is calculated without any possible priors on
parameter $i$. The correlation coefficient between two parameters $i$ and $j$
can be calculated as:
\begin{equation}
 c_{ij}
   = \frac{\left(\mathcal{F}^{-1}\right)_{ij}}{\sigma_i \sigma_j}
   = \frac{\sigma_{ij}}{\sigma_i \sigma_j}
 \label{eqn:corr_coeff}
\end{equation}
With this, the error ellipse in the $(p_i,p_j)$ plane can be calculated as
well. Its semi-major and semi-minor axes $a$ and $b$ are given by the
eigenvalues of the $2\times 2$ sub-matrix of the covariance matrix corresponding
to the two parameters,
\begin{equation}
 \mathcal{S}^{(i,j)} =
 \begin{pmatrix}
  \,\sigma_i^2 & \sigma_{ij}\, \\
  \,\sigma_{ij} & \sigma_j^2\,
 \end{pmatrix} \quad,
 \label{eqn:cov_submatrix}
\end{equation}
which are equal to \cite{Fisher_ellipse}
\begin{eqnarray}
 a^2 &=& \frac{\sigma_i^2 + \sigma_j^2}{2} +
         \sqrt{\frac{\left(\sigma_i^2+\sigma_j^2\right)^2}{4}+\sigma_{ij}^2} \\
 b^2 &=& \frac{\sigma_i^2 + \sigma_j^2}{2} -
         \sqrt{\frac{\left(\sigma_i^2+\sigma_j^2\right)^2}{4} + \sigma_{ij}^2} 
\end{eqnarray}
for $\sigma_i > \sigma_j$. Otherwise, the roles of semi-major and semi-minor
axis have to be exchanged. These values have to be scaled by a factor $\alpha$
reflecting the confidence level the ellipse is supposed to represent, \eg
$\mathrm{CL} = 68\,\%$ for a $1\sigma$ ellipse. It can be determined after
\begin{equation}
 \alpha = \sqrt{\mathrm{PPF}_{\chi^2,\,2}(\mathrm{CL})} \quad,
\end{equation}
where $\mathrm{PPF}_{\chi^2,\,2}$ is the \emph{percent point function}---the
inverse of the cumulative distribution function---for the $\chi^2$ distribution
with two degrees of freedom.
The rotation of the ellipse corresponds to the angle $\phi$ between the $p_i$
axis and the eigenvector of the reduced covariance matrix
(\ref{eqn:cov_submatrix}):
\begin{equation}
 \tan 2\phi = \frac{2\sigma_{ij}}{\sigma_i^2 - \sigma_j^2}
 \label{eqn:ell_angle}
\end{equation}
Since the parameters $p_i$ and $p_j$ do not necessarily have the same
dimension, all values in the equations above have to taken as dimensionless,
\ie in units of actual size on a plot. For this reason, (\ref{eqn:ell_angle})
has to be divided by the aspect ratio of the plot before taking the arc tangent.

Another important property of the Fisher matrix is its additivity. As one can
see from (\ref{eqn:fisher_def}), where the total Fisher matrix is given by 
summing the contributions from all observables, one can always add a new
measurement to the ones already accounted for just by adding the respective
Fisher matrices. If there are parameters that only appear in one of the
measurements---\eg the relative atmospheric neutrino flux scale
$r_{\mathrm{flux},\,\nue-\numu}$ that has to be considered in PINGU, but not in
JUNO as the latter relies on reactor neutrinos---the matrix of the experiment
not yet having said parameter is expanded by a corresponding row and column
filled with zeroes: As the measurement does not depend on the new parameter, the
uncertainty on its value as extracted from the measurement
(\ref{eqn:sigma_stat}) is $1/\sqrt{0} = \infty$. The full covariance matrix of
the combination of the two experiments is then simply given by the inverse of
the added Fisher matrices.

\subsection{Prerequisites}
\label{sec:fisher_prereq}

As the simplicity of the Fisher matrix formalism suggests, it is not
universally applicable, but an approximation that is only valid under certain
circumstances. The basic requirement that has to be met is that the fiducial
model, in which the Fisher matrix is constructed, already gives a good
description of the actual ``truth''. There are several criteria to judge
whether this requirement is in fact fulfilled, which turn out to all be
equivalent:
\begin{enumerate}[(a)]
 \item In parameter space, the fiducial and true points have to be close
  enough, such that the dependence of the observables on the parameters,
  $f_n(p_i)$, can be approximated by linear functions. I.\,e.\ the partial
  derivatives $\frac{\partial f_n}{\partial p_i}$ entering in
  (\ref{eqn:fisher_def}) are the same at both points. If this was not the case,
  the experiment's ``response'' to the parameters would be different at the two
  points and hence the fiducial point was not appropriate to make predictions
  about the actual truth.
 \item When constructing the likelihood landscape over the parameter space, such
  that
  \begin{equation}
   \frac{\partial^2\mathcal{L}}{\partial p_i^2}
     = \sum_n \frac{\partial f_n}{\partial p_i}
   \label{eqn:fisher_llh}
  \end{equation}
  with a minimum of $\mathcal{L}$ at the true point, at the fiducial point the
  shape of $\mathcal{L}$ can still be approximated by a parabola with the apex
  at the true point. This ensures that the error ellipses, which are nothing
  else than iso-likelihood contours, are in fact elliptical.
 \item The test statistics of the experiment is Gaussian. This means that the
  likelihood values of an ensemble of pseudo-experiments follow a Gaussian
  distribution, no matter whether they are thrown at the fiducial or at the true
  point. \emph{Throwing a pseudo-experiment} means in this case that
  a possible ``real'' experimental outcome is simulated by generating a 2D
  histogram of events filled with random numbers following a Poisson
  distribution with the means taken from the expected event numbers generated
  with \papa.
\end{enumerate}
% Since the likelihood $\mathcal{L}$ introduced in (\ref{eqn:fisher_llh}) does
% not need to be constructed explicitly, only condition (a) will be checked to
% be fulfilled. This validation can be found in App.~\ref{app:fisher_valid}.
That these conditions are, in fact, fulfilled for the baseline settings of
PINGU is demonstrated in App.~\ref{app:fisher_valid}. As for conditions (b)
and (c) this would require a likelihood evaluation of the full parameter
space\footnote{This is not possible within reasonable computing time, which is
why the Fisher Matrix approach has been chosen in the first place.}, here the
validation is restricted to a case with only two systematic parameters, \dm{31}
and \thet{23}.

\subsection{The Hierarchy Parameter}
\label{sec:fisher_hierarchy}

The Fisher matrix can only handle continuous parameters $p_i$, as the
derivatives of the observables \wrt these parameters enter in
(\ref{eqn:fisher_def}). For this reason the hierarchy parameter $h$ was
introduced in (\ref{eqn:hierarchy_parameter}), making the intrinsically
binary\footnote{By construction it can only be either normal or inverted.}
neutrino mass hierarchy continuous. However it still needs to be demonstrated
that this definition leads to a result that can be interpreted in a meaningful
way.

In \cite{Akhmedov}, a metric is defined that allows to evaluate the sensitivity
of an atmospheric neutrino experiment to the mass hierarchy in terms of
standard \delchisq statistics. For every bin of the final analysis histograms in
($E,\coszen$), a \delchi is defined as
\begin{equation}
 \delchi = \frac{N_\mathrm{NH}-N_\mathrm{IH}}{\sqrt{N_\mathrm{NH}}} \quad.
 \label{eqn:def_delchi}
\end{equation}
The expected significance of the experiment's mass hierarchy measurement is
then given by
\begin{equation}
 \mathcal{S}_{\chi^2} = \sqrt{\sum_\mathrm{bins} \delchisq}
 = \sqrt{\sum_\mathrm{bins}
     \frac{(N_\mathrm{NH}-N_\mathrm{IH})^2}{N_\mathrm{NH}} }\quad.
\end{equation}
With the definition of $h$ according to (\ref{eqn:hierarchy_parameter}), from
the Fisher Matrix the statistical error on $h$ can be retrieved:
\begin{equation}
 \sigma_h^\mathrm{stat} = \sqrt{\left(\mathcal{F}_{hh}\right)^{-1}} \quad,
\end{equation}
with
\begin{equation}
 \mathcal{F}_{hh} = \sum_\mathrm{bins} \frac{1}{\sigma_b^2}
   \frac{\partial N_b}{\partial h} \frac{\partial N_b}{\partial h}
  = \sum_\mathrm{bins} \frac{1}{N_\mathrm{NH}}(N_\mathrm{NH}-N_\mathrm{IH})^2
  \quad.
\end{equation}
As the pure normal and pure inverted hierarchy case correspond to $h=1$ and
$h=0$, respectively, \ie the two cases that have to be distinguished differ by
$1$, the statistical significance (without including any systematic parameters)
retrieved from the Fisher matrix is
\begin{equation}
 \mathcal{S}_\mathrm{Fisher} = \frac{1}{\sigma_h^\mathrm{stat}} 
 = \sqrt{\mathcal{F}_{hh}}
 = \sqrt{\sum_\mathrm{bins}
     \frac{(N_\mathrm{NH}-N_\mathrm{IH})^2}{N_\mathrm{NH}}}
 = \mathcal{S}_{\chi^2} \quad.
\end{equation}
Thus the significances calculated according to \delchisq statistics, where by
construction no systematic parameters are included, and from the Fisher matrix
with explicitly excluding systematics are, in fact, identical.

\subsection{Constructing the Fisher Matrix with \papa}

To construct the Fisher matrix for PINGU primarily means to derive the partial
derivatives entering (\ref{eqn:fisher_def}), \ie the derivatives of the
expected number of events \wrt to all systematic parameters $p_i$ introduced in
Sec.~\ref{sec:systematics} for each bin in the final analysis histograms. In
order to this, for every parameter a set of test points\footnote{Typically
$\approx 10$ different values.} in the vicinity of its fiducial value and \papa
is run for this test point while all other parameters are kept at their fiducial
values.

Then for every bin $b$ in the analysis histograms, the expected bin count $N_b$
is fitted as a function of $p_i$ with a parabola, \ie
\begin{equation}
 N_b^\mathrm{fit}(p_i) = c_0 + c_1\cdot p_i + c_2\cdot p_i^2 \quad.
\end{equation}
The sought-after derivative is then simply given by
\begin{equation}
 \frac{\partial N_b^\mathrm{fit}}{\partial p_i} = c_1 + 2 c_2 p_i 
\end{equation}
with $p_i$ at the fiducial value. As this procedure is repeated for ever
parameter and every bin, it produces all ingredients needed to finally
construct the Fisher matrix.

Here another advantage of the Fisher matrix formalism becomes obvious: since
all systematic parameters are treated independently from each other, adding a
new parameter results only in a small relative increase in computing time.
The parameter space is sampled only along one-dimensional paths aligned with
the parameter axes, thus adding an extra dimension only means adding one path
to the sampling. Hence the time needed to get a result scales only linearly
with the number of parameters taken into account while for algorithms sampling
the full parameter space the scaling is exponential. Using a more graphic
explanation, the Fisher matrix infers the shape of the likelihood landscape by
retrieving a cross-section in each coordinate direction and then ``rotates''
them assuming a parabolic shape.